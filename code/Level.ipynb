{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(s):\n",
    "    return round(np.sum(s) / len(s)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgLV(data):\n",
    "    tags = data['KnowledgeTag'].unique()\n",
    "    LV = []\n",
    "    \n",
    "    for tid in tags:\n",
    "        LV.append(data[data['KnowledgeTag']==tid]['userLV_Tag'].iloc[0])\n",
    "    return round(np.mean(LV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "Total Data: 2513268\n",
      "Feature Engineering Data (only answerCode >= 0 -> train data): 2513268\n",
      "Calculating correct tagRatio and userBytagRatio ...\n",
      " Calculating correct userRatio ...\n",
      "Done!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading Data ...')\n",
    "total = pd.read_csv('../data/total_data.csv')\n",
    "train = total[total['answerCode']>=0]\n",
    "print(f'Total Data: {len(total)}')\n",
    "print(f'Feature Engineering Data (only answerCode >= 0 -> train data): {len(train)}')\n",
    "\n",
    "# 1. 태그별 정답률 & 유저별 태그 정답률\n",
    "print(f'Calculating correct tagRatio and userBytagRatio ...')\n",
    "train['tagRatio'] = train.groupby('KnowledgeTag').answerCode.transform(percentile)\n",
    "train['userBytagRatio'] = train.groupby(['userID', 'KnowledgeTag']).answerCode.transform(percentile)\n",
    "\n",
    "# 2.유저별 정답률\n",
    "print(f' Calculating correct userRatio ...')\n",
    "train['userRatio'] = train.groupby('userID').answerCode.transform(percentile)\n",
    "print(f'Done!!\\n')\n",
    "\n",
    "feature_by_test = train.drop(['assessmentItemID', 'Timestamp', 'testId', 'answerCode'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KnowledgeTag Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling for Tag Level...\n",
      "tag mean ratio: 61.65%\n",
      "diff min(-44.05%), diff max(35.59%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tag_mean_ratio = round(feature_by_test.groupby('KnowledgeTag').mean()['tagRatio'].mean(), 2)\n",
    "diff =  feature_by_test['tagRatio'] - tag_mean_ratio\n",
    "pre_std = int(diff.min()//10)*10\n",
    "last_std = int(diff.max()//10+1)*10\n",
    "LV = (last_std - pre_std)//10\n",
    "\n",
    "print('Labeling for Tag Level...')\n",
    "print(f'tag mean ratio: {tag_mean_ratio}%')\n",
    "print(f'diff min({round(diff.min(), 2)}%), diff max({round(diff.max(), 2)}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 94.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50% < diff <= -40%  |  Level: 9\n",
      "-40% < diff <= -30%  |  Level: 8\n",
      "-30% < diff <= -20%  |  Level: 7\n",
      "-20% < diff <= -10%  |  Level: 6\n",
      "-10% < diff <=   0%  |  Level: 5\n",
      "  0% < diff <=  10%  |  Level: 4\n",
      " 10% < diff <=  20%  |  Level: 3\n",
      " 20% < diff <=  30%  |  Level: 2\n",
      " 30% < diff <=  40%  |  Level: 1\n",
      "\n",
      "Done!!\n",
      "Check all Tag Level: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for std in tqdm(range(pre_std+10, last_std+1, 10)):\n",
    "    print(f'{pre_std:3d}% < diff <= {std:3d}%  |  Level: {LV}')\n",
    "    idx = (pre_std < diff) & (diff <= std)\n",
    "    feature_by_test.loc[idx, 'tagLV'] = LV\n",
    "    pre_std = std\n",
    "    LV -= 1\n",
    "\n",
    "print()\n",
    "print('Done!!')\n",
    "print(f'Check all Tag Level: {sorted(feature_by_test[\"tagLV\"].unique())}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Level by KnowledgeTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling for User Level by Tag...\n",
      "diff min(-97.24%), diff max(82.4%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff =  feature_by_test['userBytagRatio'] - feature_by_test['tagRatio']\n",
    "pre_std = int(diff.min()//10)*10\n",
    "last_std = int(diff.max()//10+1)*10\n",
    "LV = 1\n",
    "\n",
    "print('Labeling for User Level by Tag...')\n",
    "print(f'diff min({round(diff.min(), 2)}%), diff max({round(diff.max(), 2)}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 11/19 [00:00<00:00, 100.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] -100% < diff <= -90%  |  Level: 1\n",
      "[DEBUG] -90% < diff <= -80%  |  Level: 2\n",
      "[DEBUG] -80% < diff <= -70%  |  Level: 3\n",
      "[DEBUG] -70% < diff <= -60%  |  Level: 4\n",
      "[DEBUG] -60% < diff <= -50%  |  Level: 5\n",
      "[DEBUG] -50% < diff <= -40%  |  Level: 6\n",
      "[DEBUG] -40% < diff <= -30%  |  Level: 7\n",
      "[DEBUG] -30% < diff <= -20%  |  Level: 8\n",
      "[DEBUG] -20% < diff <= -10%  |  Level: 9\n",
      "[DEBUG] -10% < diff <=   0%  |  Level: 10\n",
      "[DEBUG]   0% < diff <=  10%  |  Level: 11\n",
      "[DEBUG]  10% < diff <=  20%  |  Level: 12\n",
      "[DEBUG]  20% < diff <=  30%  |  Level: 13\n",
      "[DEBUG]  30% < diff <=  40%  |  Level: 14\n",
      "[DEBUG]  40% < diff <=  50%  |  Level: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 101.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]  50% < diff <=  60%  |  Level: 16\n",
      "[DEBUG]  60% < diff <=  70%  |  Level: 17\n",
      "[DEBUG]  70% < diff <=  80%  |  Level: 18\n",
      "[DEBUG]  80% < diff <=  90%  |  Level: 19\n",
      "\n",
      "Done!!\n",
      "Check all User Level: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for std in tqdm(range(pre_std+10, last_std+1, 10)):\n",
    "    print(f'[DEBUG] {pre_std:3d}% < diff <= {std:3d}%  |  Level: {LV}')\n",
    "    idx = (pre_std < diff) & (diff <= std)\n",
    "    feature_by_test.loc[idx, 'userLV'] = LV\n",
    "    pre_std = std\n",
    "    LV += 1\n",
    "\n",
    "print()\n",
    "print('Done!!')\n",
    "print(f'Check all User Level: {sorted(feature_by_test[\"userLV\"].unique())}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling for Total User Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling for Total User Level (tagLV x userLV) ...\n",
      "Done!!\n",
      "Num of User Level: 67\n",
      "Check all User Level: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 14.0, 15.0, 16.0, 18.0, 20.0, 21.0, 22.0, 24.0, 25.0, 27.0, 28.0, 30.0, 32.0, 33.0, 35.0, 36.0, 39.0, 40.0, 42.0, 44.0, 45.0, 48.0, 49.0, 50.0, 52.0, 54.0, 55.0, 56.0, 60.0, 63.0, 64.0, 65.0, 66.0, 70.0, 72.0, 75.0, 77.0, 78.0, 80.0, 81.0, 84.0, 88.0, 90.0, 91.0, 96.0, 98.0, 104.0, 105.0, 112.0, 119.0, 120.0, 126.0, 136.0, 144.0, 171.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Labeling for Total User Level (tagLV x userLV) ...')\n",
    "feature_by_test['userLV_Tag'] = feature_by_test['tagLV']*feature_by_test['userLV']\n",
    "print('Done!!')\n",
    "print(f'Num of User Level: {len(feature_by_test[\"userLV_Tag\"].unique())}')\n",
    "print(f'Check all User Level: {sorted(feature_by_test[\"userLV_Tag\"].unique())}\\n')\n",
    "\n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling for User Average Level ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6844/6844 [00:42<00:00, 161.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n",
      "Num of User AVG Levels: 70\n",
      "Level(min): 11.0, Level(max): 91.0\n",
      "Check all User AVG Level: [11.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 84.0, 91.0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Labeling for User Average Level ...')\n",
    "user_grouby = feature_by_test.groupby('userID').apply(avgLV)\n",
    "for uid in tqdm(user_grouby.index):\n",
    "    feature_by_test.loc[feature_by_test['userID']==uid, 'userLV_Tag_avg'] = user_grouby.loc[uid]\n",
    "levels = feature_by_test['userLV_Tag_avg'].unique()\n",
    "print(f'Done!!')\n",
    "print(f'Num of User AVG Levels: {len(levels)}')\n",
    "print(f'Level(min): {min(levels)}, Level(max): {max(levels)}')\n",
    "print(f'Check all User AVG Level: {sorted(levels)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Total Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge with Total Dataset ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  9.90it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f'Merge with Total Dataset ...\\n')\n",
    "idx = feature_by_test.index\n",
    "columns = ['tagLV', 'userLV_Tag_avg', 'userLV_Tag']\n",
    "for column in tqdm(columns):\n",
    "    total.loc[idx, column] = feature_by_test[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering to Inference data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature Engineering to Inference data ...')\n",
    "test = total[total['answerCode']<0]\n",
    "for idx in tqdm(test.index):\n",
    "    uid = test.loc[idx, 'userID']\n",
    "    tagid = test.loc[idx, 'KnowledgeTag']\n",
    "    total.loc[idx, 'tagLV'] = total[total['KnowledgeTag']==tagid]['tagLV'].iloc[0]\n",
    "    avg = total[total['userID']==uid]['userLV_Tag_avg'].iloc[0]\n",
    "    lv = total[(total['userID']==uid)&(total['KnowledgeTag']==tagid)]['userLV_Tag'].iloc[0]\n",
    "    if np.isnan(lv): # 유저가 tag를 한 번도 풀어본 적이 없는 경우, 유저의 평균 lv를 넣어줌 \n",
    "        total.loc[idx, 'userLV_Tag'] = avg\n",
    "    else:\n",
    "        total.loc[idx, 'userLV_Tag'] = lv\n",
    "    total.loc[idx, 'userLV_Tag_avg'] = avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Done!!\n",
      "Check your \"../data/total_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "print(f'Saving ...')\n",
    "total = total.sort_values(by=[\"userID\", \"Timestamp\"]).reset_index(drop=True)\n",
    "total.to_csv('../data/total_data_2.csv', index=False)\n",
    "print('Done!!')\n",
    "print(f'Check your \"../data/total_data.csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'assessmentItemID', 'testId', 'answerCode', 'Timestamp',\n",
       "       'KnowledgeTag', 'train', 'same_item_cnt', 'prior_elapsed',\n",
       "       'current_elapsed', 'timeClass', 'day_diff', 'Bigcat', 'user_avg',\n",
       "       'test_avg', 'item_avg', 'Bigcat_avg', 'tag_avg', 'user_time_avg',\n",
       "       'test_time_avg', 'item_time_avg', 'Bigcat_time_avg', 'tag_time_avg',\n",
       "       'user_std', 'test_std', 'item_std', 'Bigcat_std', 'tag_std',\n",
       "       'user_correct_answer', 'user_total_answer', 'user_Cumacc',\n",
       "       'user_Bigcat_correct_answer', 'user_Bigcat_total_answer',\n",
       "       'user_Bigcat_Cumacc', 'user_current_avg', 'user_current_time_avg',\n",
       "       'Bigcat_class', 'assess_count', 'tag_count', 'item_seq',\n",
       "       'user_retCount_correct_answer', 'user_retCount', 'user_retCumacc',\n",
       "       'elo', 'KnowledgeTag_elo', 'Bigcat_elo', 'tagLV', 'userLV_Tag_avg',\n",
       "       'userLV_Tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userID                          0\n",
       "assessmentItemID                0\n",
       "testId                          0\n",
       "answerCode                      0\n",
       "Timestamp                       0\n",
       "KnowledgeTag                    0\n",
       "train                           0\n",
       "same_item_cnt                   0\n",
       "prior_elapsed                   0\n",
       "current_elapsed                 0\n",
       "timeClass                       0\n",
       "day_diff                        0\n",
       "Bigcat                          0\n",
       "user_avg                        0\n",
       "test_avg                        0\n",
       "item_avg                        0\n",
       "Bigcat_avg                      0\n",
       "tag_avg                         0\n",
       "user_time_avg                   0\n",
       "test_time_avg                   0\n",
       "item_time_avg                   0\n",
       "Bigcat_time_avg                 0\n",
       "tag_time_avg                    0\n",
       "user_std                        0\n",
       "test_std                        0\n",
       "item_std                        0\n",
       "Bigcat_std                      0\n",
       "tag_std                         0\n",
       "user_correct_answer             0\n",
       "user_total_answer               0\n",
       "user_Cumacc                     0\n",
       "user_Bigcat_correct_answer      0\n",
       "user_Bigcat_total_answer        0\n",
       "user_Bigcat_Cumacc              0\n",
       "user_current_avg                0\n",
       "user_current_time_avg           0\n",
       "Bigcat_class                    0\n",
       "assess_count                    0\n",
       "tag_count                       0\n",
       "item_seq                        0\n",
       "user_retCount_correct_answer    0\n",
       "user_retCount                   0\n",
       "user_retCumacc                  0\n",
       "elo                             0\n",
       "KnowledgeTag_elo                0\n",
       "Bigcat_elo                      0\n",
       "tagLV                           0\n",
       "userLV_Tag_avg                  0\n",
       "userLV_Tag                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('DKT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98e00b988309a6a2b4bf5da131e8a00bb5c1d7ca57880f6a90a6545e6b31c465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
